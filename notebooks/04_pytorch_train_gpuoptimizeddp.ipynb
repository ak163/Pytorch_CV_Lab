{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67686b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbgiddwani\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbgiddwani\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbgiddwani\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbgiddwani\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdark-disco-33\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/bgiddwani/pytorch-lab\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/bgiddwani/pytorch-lab/runs/3vf85f1w\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /workspace/home/Pytorch_Lab/notebooks/wandb/run-20220114_192909-3vf85f1w\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfearless-lion-35\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/bgiddwani/pytorch-lab\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/bgiddwani/pytorch-lab/runs/2js7w69j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /workspace/home/Pytorch_Lab/notebooks/wandb/run-20220114_192909-2js7w69j\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvivid-shape-34\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/bgiddwani/pytorch-lab\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/bgiddwani/pytorch-lab/runs/1a3f8ca5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /workspace/home/Pytorch_Lab/notebooks/wandb/run-20220114_192909-1a3f8ca5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mleafy-dream-36\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/bgiddwani/pytorch-lab\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/bgiddwani/pytorch-lab/runs/nbuyhiza\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /workspace/home/Pytorch_Lab/notebooks/wandb/run-20220114_192909-nbuyhiza\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Epoch 0/9\n",
      "----------\n",
      "Epoch 0/9\n",
      "----------\n",
      "Epoch 0/9\n",
      "----------\n",
      "Epoch 0/9\n",
      "----------\n",
      "Validation Loss is 8.53211001304731\n",
      "Validation Accuracy is 0.452054794520548\n",
      "Epoch 1/9\n",
      "----------\n",
      "Validation Loss is 8.53211001304731\n",
      "Validation Accuracy is 0.452054794520548\n",
      "Epoch 1/9\n",
      "----------\n",
      "Validation Loss is 8.53211001304731\n",
      "Validation Accuracy is 0.452054794520548\n",
      "Epoch 1/9\n",
      "----------\n",
      "Validation Loss is 8.53211001304731\n",
      "Validation Accuracy is 0.452054794520548\n",
      "Epoch 1/9\n",
      "----------\n",
      "Loss after 00728 examples: 22098.250\n",
      "Loss after 00728 examples: 27606.117Loss after 00728 examples: 14379.605\n",
      "\n",
      "Loss after 00728 examples: 28794.242\n",
      "Validation Loss is 0.7772894964438595\n",
      "Validation Accuracy is 0.8684931506849315\n",
      "Epoch 2/9\n",
      "----------\n",
      "Validation Loss is 0.7772894964438595\n",
      "Validation Accuracy is 0.8684931506849315\n",
      "Epoch 2/9\n",
      "----------\n",
      "Validation Loss is 0.7772894964438595\n",
      "Validation Accuracy is 0.8684931506849315\n",
      "Epoch 2/9\n",
      "----------\n",
      "Validation Loss is 0.7772894964438595\n",
      "Validation Accuracy is 0.8684931506849315\n",
      "Epoch 2/9\n",
      "----------\n",
      "Validation Loss is 0.3808469195888467\n",
      "Validation Accuracy is 0.8904109589041096\n",
      "Epoch 3/9\n",
      "----------\n",
      "Validation Loss is 0.3808469195888467\n",
      "Validation Accuracy is 0.8904109589041096\n",
      "Epoch 3/9\n",
      "----------\n",
      "Validation Loss is 0.3808469195888467\n",
      "Validation Accuracy is 0.8904109589041096\n",
      "Epoch 3/9\n",
      "----------\n",
      "Validation Loss is 0.3808469195888467\n",
      "Validation Accuracy is 0.8904109589041096\n",
      "Epoch 3/9\n",
      "----------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Validation Loss is 0.34667984315589684\n",
      "Validation Accuracy is 0.9205479452054794\n",
      "Epoch 4/9\n",
      "----------\n",
      "Validation Loss is 0.34667984315589684\n",
      "Validation Accuracy is 0.9205479452054794\n",
      "Epoch 4/9\n",
      "----------\n",
      "Validation Loss is 0.34667984315589684\n",
      "Validation Accuracy is 0.9205479452054794\n",
      "Epoch 4/9\n",
      "----------\n",
      "Validation Loss is 0.34667984315589684\n",
      "Validation Accuracy is 0.9205479452054794\n",
      "Epoch 4/9\n",
      "----------\n",
      "Loss after 01488 examples: 13934.133\n",
      "Loss after 01488 examples: 13560.309\n",
      "Loss after 01488 examples: 12306.168\n",
      "Loss after 01488 examples: 8589.711\n",
      "Validation Loss is 0.5480720676787912\n",
      "Validation Accuracy is 0.8465753424657534\n",
      "Epoch 5/9\n",
      "----------\n",
      "Validation Loss is 0.5480720676787912\n",
      "Validation Accuracy is 0.8465753424657534\n",
      "Epoch 5/9\n",
      "----------\n",
      "Validation Loss is 0.5480720676787912\n",
      "Validation Accuracy is 0.8465753424657534\n",
      "Epoch 5/9\n",
      "----------\n",
      "Validation Loss is 0.5480720676787912\n",
      "Validation Accuracy is 0.8465753424657534\n",
      "Epoch 5/9\n",
      "----------\n",
      "Validation Loss is 1.0021966578209236\n",
      "Validation Accuracy is 0.6931506849315069\n",
      "Epoch 6/9\n",
      "----------\n",
      "Validation Loss is 1.0021966578209236\n",
      "Validation Accuracy is 0.6931506849315069\n",
      "Epoch 6/9\n",
      "----------\n",
      "Validation Loss is 1.0021966578209236\n",
      "Validation Accuracy is 0.6931506849315069\n",
      "Epoch 6/9\n",
      "----------\n",
      "Validation Loss is 1.0021966578209236\n",
      "Validation Accuracy is 0.6931506849315069\n",
      "Epoch 6/9\n",
      "----------\n",
      "Loss after 02248 examples: 13730.415\n",
      "Loss after 02248 examples: 8121.052\n",
      "Loss after 02248 examples: 16161.601\n",
      "Loss after 02248 examples: 13297.685\n",
      "Validation Loss is 0.2755059060168593\n",
      "Validation Accuracy is 0.9178082191780822\n",
      "Epoch 7/9\n",
      "----------\n",
      "Validation Loss is 0.2755059060168593\n",
      "Validation Accuracy is 0.9178082191780822\n",
      "Epoch 7/9\n",
      "----------\n",
      "Validation Loss is 0.2755059060168593\n",
      "Validation Accuracy is 0.9178082191780822\n",
      "Epoch 7/9\n",
      "----------\n",
      "Validation Loss is 0.2755059060168593\n",
      "Validation Accuracy is 0.9178082191780822\n",
      "Epoch 7/9\n",
      "----------\n",
      "Validation Loss is 0.2661230342437143\n",
      "Validation Accuracy is 0.9205479452054794\n",
      "Epoch 8/9\n",
      "----------\n",
      "Validation Loss is 0.2661230342437143\n",
      "Validation Accuracy is 0.9205479452054794\n",
      "Epoch 8/9\n",
      "----------\n",
      "Validation Loss is 0.2661230342437143\n",
      "Validation Accuracy is 0.9205479452054794\n",
      "Epoch 8/9\n",
      "----------\n",
      "Validation Loss is 0.2661230342437143\n",
      "Validation Accuracy is 0.9205479452054794\n",
      "Epoch 8/9\n",
      "----------\n",
      "Loss after 03008 examples: 19778.047\n",
      "Loss after 03008 examples: 13803.340\n",
      "Loss after 03008 examples: 13259.445\n",
      "Loss after 03008 examples: 7656.846\n",
      "Validation Loss is 0.26928054616875846\n",
      "Validation Accuracy is 0.9178082191780822\n",
      "Epoch 9/9\n",
      "----------\n",
      "Validation Loss is 0.26928054616875846\n",
      "Validation Accuracy is 0.9178082191780822\n",
      "Epoch 9/9\n",
      "----------\n",
      "Validation Loss is 0.26928054616875846\n",
      "Validation Accuracy is 0.9178082191780822\n",
      "Epoch 9/9\n",
      "----------\n",
      "Validation Loss is 0.26928054616875846\n",
      "Validation Accuracy is 0.9178082191780822\n",
      "Epoch 9/9\n",
      "----------\n",
      "Validation Loss is 0.2613561116261025\n",
      "Validation Accuracy is 0.9287671232876712\n",
      "Training complete in 1m 33s\n",
      "Validation Loss is 0.2613561116261025\n",
      "Validation Accuracy is 0.9287671232876712\n",
      "Training complete in 1m 33s\n",
      "Validation Loss is 0.2613561116261025\n",
      "Validation Accuracy is 0.9287671232876712\n",
      "Training complete in 1m 33s\n",
      "Validation Loss is 0.2613561116261025\n",
      "Validation Accuracy is 0.9287671232876712\n",
      "Training complete in 1m 33s\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 28091\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.00MB of 0.00MB uploaded (0.00MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 28087\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 28085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.00MB of 0.00MB uploaded (0.00MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 28079\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /workspace/home/Pytorch_Lab/notebooks/wandb/run-20220114_192909-2js7w69j/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /workspace/home/Pytorch_Lab/notebooks/wandb/run-20220114_192909-2js7w69j/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                    test_accuracy 0.92877\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                         _runtime 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                       _timestamp 1642188649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                            _step 3031\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                            epoch 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                             loss 13259.44531\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_accuracy ▁▁▁▁▇▇▇▇▆▇▇▇▇▇▇▇▇▇▆▆▄▄▅▅▇▇▇███▇█▇▇██▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           _step ▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▆▆▆▆▆▆▆▆████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch ▁▄▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            loss █▁▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfearless-lion-35\u001b[0m: \u001b[34mhttps://wandb.ai/bgiddwani/pytorch-lab/runs/2js7w69j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /workspace/home/Pytorch_Lab/notebooks/wandb/run-20220114_192909-nbuyhiza/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /workspace/home/Pytorch_Lab/notebooks/wandb/run-20220114_192909-nbuyhiza/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                    test_accuracy 0.92877\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                         _runtime 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                       _timestamp 1642188649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                            _step 3031\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                            epoch 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                             loss 13803.33984\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_accuracy ▁▁▁▁▇▇▇▇▆▇▇▇▇▇▇▇▇▇▆▆▄▄▅▅▇▇▇███▇█▇▇██▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           _step ▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▆▆▆▆▆▆▆▆████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch ▁▄▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            loss █▁▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mleafy-dream-36\u001b[0m: \u001b[34mhttps://wandb.ai/bgiddwani/pytorch-lab/runs/nbuyhiza\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /workspace/home/Pytorch_Lab/notebooks/wandb/run-20220114_192909-1a3f8ca5/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /workspace/home/Pytorch_Lab/notebooks/wandb/run-20220114_192909-1a3f8ca5/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                    test_accuracy 0.92877\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                         _runtime 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                       _timestamp 1642188649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                            _step 3031\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                            epoch 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                             loss 19778.04688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_accuracy ▁▁▁▁▇▇▇▇▆▇▇▇▇▇▇▇▇▇▆▆▄▄▅▅▇▇▇███▇█▇▇██▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           _step ▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▆▆▆▆▆▆▆▆████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch ▁▄▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            loss █▁▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mvivid-shape-34\u001b[0m: \u001b[34mhttps://wandb.ai/bgiddwani/pytorch-lab/runs/1a3f8ca5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /workspace/home/Pytorch_Lab/notebooks/wandb/run-20220114_192909-3vf85f1w/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /workspace/home/Pytorch_Lab/notebooks/wandb/run-20220114_192909-3vf85f1w/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                    test_accuracy 0.92877\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                         _runtime 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                       _timestamp 1642188649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                            _step 3031\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                            epoch 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                             loss 7656.84619\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   test_accuracy ▁▁▁▁▇▇▇▇▆▇▇▇▇▇▇▇▇▇▆▆▄▄▅▅▇▇▇███▇█▇▇██▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           _step ▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▆▆▆▆▆▆▆▆████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch ▁▄▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            loss █▃▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdark-disco-33\u001b[0m: \u001b[34mhttps://wandb.ai/bgiddwani/pytorch-lab/runs/3vf85f1w\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 02_pytorch_train_gpuoptimizeddp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf9672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
