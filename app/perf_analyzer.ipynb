{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a59e14",
   "metadata": {},
   "source": [
    "## Server Configuration Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b85033",
   "metadata": {},
   "source": [
    "### The below code will check the health of the Triton Inference Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f2c97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/health/ready HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Length: 0\n",
      "< Content-Type: text/plain\n",
      "< \n",
      "* Connection #0 to host localhost left intact\n"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dcd7d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/models/resnet50_torch HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 210\n",
      "< \n",
      "* Connection #0 to host localhost left intact\n",
      "{\"name\":\"resnet50_torch\",\"versions\":[\"1\"],\"platform\":\"pytorch_libtorch\",\"inputs\":[{\"name\":\"input__0\",\"datatype\":\"FP32\",\"shape\":[-1,3,224,224]}],\"outputs\":[{\"name\":\"output__0\",\"datatype\":\"FP32\",\"shape\":[-1,4]}]}"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/models/resnet50_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a9a903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/models/resnet50_onnx HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 203\n",
      "< \n",
      "* Connection #0 to host localhost left intact\n",
      "{\"name\":\"resnet50_onnx\",\"versions\":[\"1\"],\"platform\":\"onnxruntime_onnx\",\"inputs\":[{\"name\":\"input\",\"datatype\":\"FP32\",\"shape\":[-1,3,224,224]}],\"outputs\":[{\"name\":\"output\",\"datatype\":\"FP32\",\"shape\":[-1,4]}]}"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/models/resnet50_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511d3273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/models/resnet50_trt_fp32 HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 204\n",
      "< \n",
      "* Connection #0 to host localhost left intact\n",
      "{\"name\":\"resnet50_trt_fp32\",\"versions\":[\"1\"],\"platform\":\"tensorrt_plan\",\"inputs\":[{\"name\":\"input\",\"datatype\":\"FP32\",\"shape\":[-1,3,224,224]}],\"outputs\":[{\"name\":\"output\",\"datatype\":\"FP32\",\"shape\":[-1,4]}]}"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/models/resnet50_trt_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6165f631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/models/resnet50_trt_fp16 HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 204\n",
      "< \n",
      "* Connection #0 to host localhost left intact\n",
      "{\"name\":\"resnet50_trt_fp16\",\"versions\":[\"1\"],\"platform\":\"tensorrt_plan\",\"inputs\":[{\"name\":\"input\",\"datatype\":\"FP32\",\"shape\":[-1,3,224,224]}],\"outputs\":[{\"name\":\"output\",\"datatype\":\"FP32\",\"shape\":[-1,4]}]}"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/models/resnet50_trt_fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd996d",
   "metadata": {},
   "source": [
    "### The below code will check the configuration file of the target model deployed on Triton Inference Server\n",
    "\n",
    "##### Syntax: !curl -v \"Server_IP\":\"Protocol_Port\"/v2/models/\"model_folder\"/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89fd0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/models/resnet50_torch/config HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 1073\n",
      "< \n",
      "{\"name\":\"resnet50_torch\",\"platform\":\"pytorch_libtorch\",\"backend\":\"pytorch\",\"version_policy\":{\"latest\":{\"num_versions\":1}},\"max_batch_size\":128,\"input\":[{\"name\":\"input__0\",\"data_type\":\"TYPE_FP32\",\"format\":\"FORMAT_NONE\",\"dims\":[3,224,224],\"is_shape_tensor\":false,\"allow_ragged_batch\":false}],\"output\":[{\"name\":\"output__0\",\"data_type\":\"TYPE_FP32\",\"dims\":[4],\"label_filename\":\"\",\"is_shape_tensor\":false}],\"batch_input\":[],\"batch_output\":[],\"optimization\":{\"priority\":\"PRIORITY_DEFAULT\",\"input_pinned_memory\":{\"enable\":true},\"output_pinned_memory\":{\"enable\":true},\"gather_kernel_buffer_threshold\":0,\"eager_batching\":false},\"dynamic_batching\":{\"preferred_batch_size\":[8,16,32],\"max_queue_delay_microseconds\":100,\"preserve_ordering\":false,\"priority_levels\":0,\"default_priority_level\":0,\"priority_queue_policy\":{}},\"instance_group\":[{\"name\":\"resnet50_torch_0\",\"kind\":\"KIND_GPU\",\"count\":1,\"gpus\":[0],\"secondary_devices\":[],\"profile\":[],\"passive\":false,\"host_policy\":\"\"}],\"default_model_filename\":\"model.pt\",\"cc_model_filenames\":{},\"m* Connection #0 to host localhost left intact\n",
      "etric_tags\":{},\"parameters\":{},\"model_warmup\":[]}"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/models/resnet50_torch/config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3414e3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/models/resnet50_onnx/config HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 1071\n",
      "< \n",
      "{\"name\":\"resnet50_onnx\",\"platform\":\"onnxruntime_onnx\",\"backend\":\"onnxruntime\",\"version_policy\":{\"latest\":{\"num_versions\":1}},\"max_batch_size\":128,\"input\":[{\"name\":\"input\",\"data_type\":\"TYPE_FP32\",\"format\":\"FORMAT_NONE\",\"dims\":[3,224,224],\"is_shape_tensor\":false,\"allow_ragged_batch\":false}],\"output\":[{\"name\":\"output\",\"data_type\":\"TYPE_FP32\",\"dims\":[4],\"label_filename\":\"\",\"is_shape_tensor\":false}],\"batch_input\":[],\"batch_output\":[],\"optimization\":{\"priority\":\"PRIORITY_DEFAULT\",\"input_pinned_memory\":{\"enable\":true},\"output_pinned_memory\":{\"enable\":true},\"gather_kernel_buffer_threshold\":0,\"eager_batching\":false},\"dynamic_batching\":{\"preferred_batch_size\":[8,16,32],\"max_queue_delay_microseconds\":100,\"preserve_ordering\":false,\"priority_levels\":0,\"default_priority_level\":0,\"priority_queue_policy\":{}},\"instance_group\":[{\"name\":\"resnet50_onnx_0\",\"kind\":\"KIND_GPU\",\"count\":1,\"gpus\":[0],\"secondary_devices\":[],\"profile\":[],\"passive\":false,\"host_policy\":\"\"}],\"default_model_filename\":\"model.onnx\",\"cc_model_filenames\":{},\"met* Connection #0 to host localhost left intact\n",
      "ric_tags\":{},\"parameters\":{},\"model_warmup\":[]}"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/models/resnet50_onnx/config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ca7f9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/models/resnet50_trt_fp32/config HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 1065\n",
      "< \n",
      "{\"name\":\"resnet50_trt_fp32\",\"platform\":\"tensorrt_plan\",\"backend\":\"\",\"version_policy\":{\"latest\":{\"num_versions\":1}},\"max_batch_size\":128,\"input\":[{\"name\":\"input\",\"data_type\":\"TYPE_FP32\",\"format\":\"FORMAT_NONE\",\"dims\":[3,224,224],\"is_shape_tensor\":false,\"allow_ragged_batch\":false}],\"output\":[{\"name\":\"output\",\"data_type\":\"TYPE_FP32\",\"dims\":[4],\"label_filename\":\"\",\"is_shape_tensor\":false}],\"batch_input\":[],\"batch_output\":[],\"optimization\":{\"priority\":\"PRIORITY_DEFAULT\",\"input_pinned_memory\":{\"enable\":true},\"output_pinned_memory\":{\"enable\":true},\"gather_kernel_buffer_threshold\":0,\"eager_batching\":false},\"dynamic_batching\":{\"preferred_batch_size\":[8,16,32],\"max_queue_delay_microseconds\":100,\"preserve_ordering\":false,\"priority_levels\":0,\"default_priority_level\":0,\"priority_queue_policy\":{}},\"instance_group\":[{\"name\":\"resnet50_trt_fp32_0\",\"kind\":\"KIND_GPU\",\"count\":1,\"gpus\":[0],\"secondary_devices\":[],\"profile\":[],\"passive\":false,\"host_policy\":\"\"}],\"default_model_filename\":\"model.plan\",\"cc_model_filenames\":{},\"metric_ta* Connection #0 to host localhost left intact\n",
      "gs\":{},\"parameters\":{},\"model_warmup\":[]}"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/models/resnet50_trt_fp32/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1805f8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/models/resnet50_trt_fp16/config HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 1065\n",
      "< \n",
      "{\"name\":\"resnet50_trt_fp16\",\"platform\":\"tensorrt_plan\",\"backend\":\"\",\"version_policy\":{\"latest\":{\"num_versions\":1}},\"max_batch_size\":128,\"input\":[{\"name\":\"input\",\"data_type\":\"TYPE_FP32\",\"format\":\"FORMAT_NONE\",\"dims\":[3,224,224],\"is_shape_tensor\":false,\"allow_ragged_batch\":false}],\"output\":[{\"name\":\"output\",\"data_type\":\"TYPE_FP32\",\"dims\":[4],\"label_filename\":\"\",\"is_shape_tensor\":false}],\"batch_input\":[],\"batch_output\":[],\"optimization\":{\"priority\":\"PRIORITY_DEFAULT\",\"input_pinned_memory\":{\"enable\":true},\"output_pinned_memory\":{\"enable\":true},\"gather_kernel_buffer_threshold\":0,\"eager_batching\":false},\"dynamic_batching\":{\"preferred_batch_size\":[8,16,32],\"max_queue_delay_microseconds\":100,\"preserve_ordering\":false,\"priority_levels\":0,\"default_priority_level\":0,\"priority_queue_policy\":{}},\"instance_group\":[{\"name\":\"resnet50_trt_fp16_0\",\"kind\":\"KIND_GPU\",\"count\":1,\"gpus\":[0],\"secondary_devices\":[],\"profile\":[],\"passive\":false,\"host_policy\":\"\"}],\"default_model_filename\":\"model.plan\",\"cc_model_filenames\":{},\"metric_ta* Connection #0 to host localhost left intact\n",
      "gs\":{},\"parameters\":{},\"model_warmup\":[]}"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/models/resnet50_trt_fp16/config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71a389",
   "metadata": {},
   "source": [
    "## Performance Benchmarking using perf_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eba638b-2afb-4de6-89fd-cca8e60634ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 128\n",
      "  Client: \n",
      "    Request count: 5047\n",
      "    Throughput: 1009.4 infer/sec\n",
      "    Avg latency: 127009 usec (standard deviation 3639 usec)\n",
      "    p50 latency: 126028 usec\n",
      "    p90 latency: 131239 usec\n",
      "    p95 latency: 133014 usec\n",
      "    p99 latency: 133705 usec\n",
      "    Avg gRPC time: 127131 usec ((un)marshal request/response 243 usec + response wait 126888 usec)\n",
      "  Server: \n",
      "    Inference count: 6021\n",
      "    Execution count: 94\n",
      "    Successful request count: 94\n",
      "    Avg request latency: 114809 usec (overhead 862 usec + queue 41724 usec + compute input 8055 usec + compute infer 64162 usec + compute output 6 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 128, throughput: 1009.4 infer/sec, latency 127009 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m resnet50_torch -b 1 -u localhost:8001 -i grpc --concurrency-range 128 --shape input__0:1,3,224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c231833b-b900-4d89-9403-75af2321626f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 128\n",
      "  Client: \n",
      "    Request count: 6912\n",
      "    Throughput: 1382.4 infer/sec\n",
      "    Avg latency: 92646 usec (standard deviation 846 usec)\n",
      "    p50 latency: 92465 usec\n",
      "    p90 latency: 93731 usec\n",
      "    p95 latency: 94435 usec\n",
      "    p99 latency: 95782 usec\n",
      "    Avg gRPC time: 92602 usec ((un)marshal request/response 252 usec + response wait 92350 usec)\n",
      "  Server: \n",
      "    Inference count: 8320\n",
      "    Execution count: 130\n",
      "    Successful request count: 130\n",
      "    Avg request latency: 79913 usec (overhead 750 usec + queue 32108 usec + compute input 6887 usec + compute infer 40016 usec + compute output 152 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 128, throughput: 1382.4 infer/sec, latency 92646 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m resnet50_onnx -b 1 -u localhost:8001 -i grpc --concurrency-range 128 --shape input:1,3,224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49531876-cfc5-4753-b7a7-b598165131cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 128\n",
      "  Client: \n",
      "    Request count: 7359\n",
      "    Throughput: 1471.8 infer/sec\n",
      "    Avg latency: 86928 usec (standard deviation 8420 usec)\n",
      "    p50 latency: 83664 usec\n",
      "    p90 latency: 96449 usec\n",
      "    p95 latency: 98208 usec\n",
      "    p99 latency: 112677 usec\n",
      "    Avg gRPC time: 86633 usec ((un)marshal request/response 220 usec + response wait 86413 usec)\n",
      "  Server: \n",
      "    Inference count: 8861\n",
      "    Execution count: 207\n",
      "    Successful request count: 207\n",
      "    Avg request latency: 75854 usec (overhead 14 usec + queue 18348 usec + compute input 25944 usec + compute infer 31500 usec + compute output 48 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 128, throughput: 1471.8 infer/sec, latency 86928 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m resnet50_trt_fp32 -b 1 -u localhost:8001 -i grpc --concurrency-range 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de3c7be7-c117-46ff-a541-8fc483f290a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 128\n",
      "  Client: \n",
      "    Request count: 18665\n",
      "    Throughput: 3733 infer/sec\n",
      "    Avg latency: 34254 usec (standard deviation 11322 usec)\n",
      "    p50 latency: 33209 usec\n",
      "    p90 latency: 43316 usec\n",
      "    p95 latency: 48723 usec\n",
      "    p99 latency: 80165 usec\n",
      "    Avg gRPC time: 34216 usec ((un)marshal request/response 248 usec + response wait 33968 usec)\n",
      "  Server: \n",
      "    Inference count: 22635\n",
      "    Execution count: 1174\n",
      "    Successful request count: 1174\n",
      "    Avg request latency: 13772 usec (overhead 8 usec + queue 3186 usec + compute input 5480 usec + compute infer 5056 usec + compute output 42 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 128, throughput: 3733 infer/sec, latency 34254 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m resnet50_trt_fp16 -b 1 -u localhost:8001 -i grpc --concurrency-range 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bb77192-b5b6-482b-9d3a-d8e1de71317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 128\n",
      "  Client: \n",
      "    Request count: 640\n",
      "    Throughput: 1024 infer/sec\n",
      "    Avg latency: 1003013 usec (standard deviation 53474 usec)\n",
      "    p50 latency: 1051187 usec\n",
      "    p90 latency: 1057124 usec\n",
      "    p95 latency: 1058654 usec\n",
      "    p99 latency: 1058950 usec\n",
      "    Avg gRPC time: 1003177 usec ((un)marshal request/response 1750 usec + response wait 1001427 usec)\n",
      "  Server: \n",
      "    Inference count: 6144\n",
      "    Execution count: 48\n",
      "    Successful request count: 48\n",
      "    Avg request latency: 979368 usec (overhead 224 usec + queue 857229 usec + compute input 16780 usec + compute infer 105128 usec + compute output 7 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 128, throughput: 1024 infer/sec, latency 1003013 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m resnet50_torch -b 8 -u localhost:8001 -i grpc --concurrency-range 128 --shape input__0:8,3,224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5feddc0e-ec6a-4801-965e-fd3808ba8bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 128\n",
      "  Client: \n",
      "    Request count: 832\n",
      "    Throughput: 1331.2 infer/sec\n",
      "    Avg latency: 768248 usec (standard deviation 42055 usec)\n",
      "    p50 latency: 743490 usec\n",
      "    p90 latency: 853769 usec\n",
      "    p95 latency: 856554 usec\n",
      "    p99 latency: 856919 usec\n",
      "    Avg gRPC time: 768645 usec ((un)marshal request/response 1863 usec + response wait 766782 usec)\n",
      "  Server: \n",
      "    Inference count: 7808\n",
      "    Execution count: 61\n",
      "    Successful request count: 61\n",
      "    Avg request latency: 745949 usec (overhead 216 usec + queue 651533 usec + compute input 16083 usec + compute infer 78068 usec + compute output 49 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 128, throughput: 1331.2 infer/sec, latency 768248 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m resnet50_onnx -b 8 -u localhost:8001 -i grpc --concurrency-range 128 --shape input:8,3,224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e49657fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 128\n",
      "  Client: \n",
      "    Request count: 1056\n",
      "    Throughput: 1689.6 infer/sec\n",
      "    Avg latency: 605870 usec (standard deviation 6908 usec)\n",
      "    p50 latency: 605895 usec\n",
      "    p90 latency: 609412 usec\n",
      "    p95 latency: 611539 usec\n",
      "    p99 latency: 642987 usec\n",
      "    Avg gRPC time: 605925 usec ((un)marshal request/response 1879 usec + response wait 604046 usec)\n",
      "  Server: \n",
      "    Inference count: 10112\n",
      "    Execution count: 79\n",
      "    Successful request count: 79\n",
      "    Avg request latency: 579238 usec (overhead 7 usec + queue 430457 usec + compute input 72988 usec + compute infer 75751 usec + compute output 35 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 128, throughput: 1689.6 infer/sec, latency 605870 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m resnet50_trt_fp32 -b 8 -u localhost:8001 -i grpc --concurrency-range 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57022c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 128\n",
      "  Client: \n",
      "    Request count: 2880\n",
      "    Throughput: 4608 infer/sec\n",
      "    Avg latency: 222197 usec (standard deviation 27151 usec)\n",
      "    p50 latency: 212855 usec\n",
      "    p90 latency: 265938 usec\n",
      "    p95 latency: 284427 usec\n",
      "    p99 latency: 315120 usec\n",
      "    Avg gRPC time: 224372 usec ((un)marshal request/response 1220 usec + response wait 223152 usec)\n",
      "  Server: \n",
      "    Inference count: 28488\n",
      "    Execution count: 701\n",
      "    Successful request count: 701\n",
      "    Avg request latency: 26870 usec (overhead 5 usec + queue 6561 usec + compute input 10789 usec + compute infer 9483 usec + compute output 32 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 128, throughput: 4608 infer/sec, latency 222197 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m resnet50_trt_fp16 -b 8 -u localhost:8001 -i grpc --concurrency-range 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8a208ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 16\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 256\n",
      "  Client: \n",
      "    Request count: 1305\n",
      "    Throughput: 4176 infer/sec\n",
      "    Avg latency: 994835 usec (standard deviation 161124 usec)\n",
      "    p50 latency: 969266 usec\n",
      "    p90 latency: 1038483 usec\n",
      "    p95 latency: 1064948 usec\n",
      "    p99 latency: 1897923 usec\n",
      "    Avg gRPC time: 992719 usec ((un)marshal request/response 3185 usec + response wait 989534 usec)\n",
      "  Server: \n",
      "    Inference count: 28560\n",
      "    Execution count: 1080\n",
      "    Successful request count: 1080\n",
      "    Avg request latency: 23443 usec (overhead 4 usec + queue 6374 usec + compute input 9238 usec + compute infer 7790 usec + compute output 37 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 256, throughput: 4176 infer/sec, latency 994835 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m resnet50_trt_fp16 -b 16 -u localhost:8001 -i grpc --concurrency-range 256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
